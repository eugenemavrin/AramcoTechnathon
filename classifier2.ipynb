{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import plaidmlkeras\n",
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../') #to load FileDataGenerator\n",
    "\n",
    "import keras\n",
    "import split_folders\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2, l1\n",
    "#kullback_leibler_divergence\n",
    "#categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "#from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "#from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "#from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "#from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, Dropout, GaussianNoise, Average, Minimum, Maximum\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import SGD, Adam, Adadelta, Nadam\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.models import load_model as klm\n",
    "import numpy as np\n",
    "#from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "import fnmatch\n",
    "import itertools\n",
    "import seaborn\n",
    "from itertools import chain\n",
    "import functools\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from FileDataGenerator import FileDataGen\n",
    "import PIL\n",
    "#import losses as custom_losses\n",
    "#from focal_loss import binary_focal_loss\n",
    "import Augmentor\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "from keras.preprocessing.image import img_to_array\n",
    "#from cycliclr import CyclicLR\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "from medpy.filter.smoothing import anisotropic_diffusion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_pm(x):\n",
    "    x = anisotropic_diffusion(x, niter=20, kappa=50, gamma=0.25)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_DIR = 'skindata3'\n",
    "TRAIN_DIR = './fortraining/train'\n",
    "TEST_DIR = './fortraining/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_folders.ratio(DB_DIR, output=\"fortraining\", seed=16, ratio=(0.8,0.2)) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "class WeightedCategoricalCrossentropy(CategoricalCrossentropy):\n",
    "    \n",
    "    def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
    "        assert(cost_mat.ndim == 2)\n",
    "        assert(cost_mat.shape[0] == cost_mat.shape[1])\n",
    "        \n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cost_mat = K.cast_to_floatx(cost_mat)\n",
    "    \n",
    "    def __call__(self, y_true, y_pred):\n",
    "      \n",
    "        return super().__call__(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
    "        )\n",
    "\n",
    "\n",
    "def get_sample_weights(y_true, y_pred, cost_m):\n",
    "    num_classes = len(cost_m)\n",
    "\n",
    "    y_pred.shape.assert_has_rank(2)\n",
    "    y_pred.shape[1].assert_is_compatible_with(num_classes)\n",
    "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
    "\n",
    "    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
    "\n",
    "    y_true_nk1 = K.expand_dims(y_true, 2)\n",
    "    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
    "    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
    "\n",
    "    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
    "    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
    "\n",
    "    return sample_weights_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_wrapper(generator):\n",
    "    for batch_x,batch_y in generator:\n",
    "        #yield (batch_x,[batch_y[:,i] for i in range(5)])\n",
    "        yield (batch_x,[batch_y, batch_y, batch_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam_dlr(optimizers.Optimizer):\n",
    "\n",
    "    \"\"\"Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        split_1: split layer 1\n",
    "        split_2: split layer 2\n",
    "        lr: float >= 0. List of Learning rates. [Early layers, Middle layers, Final Layers]\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_1, split_2, lr=[1e-7, 1e-4, 1e-2], beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n",
    "        super(Adam_dlr, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # Extracting name of the split layers\n",
    "            #self.split_1 = split_1.weights[0].name\n",
    "            #self.split_2 = split_2.weights[0].name\n",
    "            self.split_1 = split_1\n",
    "            self.split_2 = split_2\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    @keras.optimizers.interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "        \n",
    "        # Setting lr of the initial layers\n",
    "        lr_grp = lr_t[0]\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            \n",
    "            # Updating lr when the split layer is encountered\n",
    "            if p.name == self.split_1:\n",
    "                lr_grp = lr_t[1]\n",
    "            if p.name == self.split_2:\n",
    "                lr_grp = lr_t[2]\n",
    "                \n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr_grp * m_t / (K.sqrt(vhat_t) + self.epsilon) # Using updated lr\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr_grp * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "#         print('Optimizer LR: ', K.get_value(self.lr))\n",
    "#         print()\n",
    "        config = {\n",
    "                  'lr': (K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(Adam_dlr, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def mergeDicts(dict1,dict2):\n",
    "    keys = set(dict1).union(dict2)\n",
    "    no = []\n",
    "    return dict((k, dict1.get(k, no) + dict2.get(k, no)) for k in keys)\n",
    "\n",
    "def plot_training(history):\n",
    "  acc = history['categorical_accuracy']\n",
    "  val_acc = history['val_categorical_accuracy']\n",
    "  loss = history['loss']\n",
    "  val_loss = history['val_loss']\n",
    "  epochs = range(len(acc))\n",
    "  \n",
    "  plt.plot(epochs, acc)\n",
    "  plt.plot(epochs, val_acc)\n",
    "  plt.legend(['train', 'validation'], loc='upper right')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss)\n",
    "  plt.plot(epochs, val_loss)\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend(['train', 'validation'], loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "train_datagen = None\n",
    "validation_datagen = None\n",
    "train_generator = None\n",
    "validation_generator = None\n",
    "nb_train_samples = None\n",
    "nb_validation_samples = None\n",
    "train_data = None\n",
    "train_labels = None\n",
    "val_data = None\n",
    "val_labels = None\n",
    "\n",
    "def initNoneDataGenerators(shuffle_train=True):\n",
    "    global train_datagen, validation_datagen, train_generator, validation_generator, nb_train_samples, nb_validation_samples\n",
    "    # prepare data augmentation configuration\n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=shuffle_train)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_train_samples = train_generator.samples\n",
    "    nb_validation_samples = validation_generator.samples\n",
    "\n",
    "def initDataGenerators(factor=1):\n",
    "    global train_datagen, validation_datagen, train_generator, validation_generator, nb_train_samples, nb_validation_samples\n",
    "    # prepare data augmentation configuration\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        rotation_range=160*factor,\n",
    "        width_shift_range=0.01*factor,\n",
    "        height_shift_range=0.01*factor,\n",
    "        shear_range=0.15*factor,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='reflect')\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        #rotation_range=30,\n",
    "        #width_shift_range=0.01,\n",
    "        #height_shift_range=0.01,\n",
    "        #shear_range=0.01,\n",
    "        #zoom_range=0.1,\n",
    "        #horizontal_flip=True,\n",
    "        fill_mode='reflect')\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_train_samples = train_generator.samples\n",
    "    nb_validation_samples = validation_generator.samples\n",
    "    \n",
    "def Read_Directory(path):\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    for class_ in os.listdir(path):\n",
    "        if not class_.startswith('.'):\n",
    "            dat = [os.path.join(path, class_, img) for img in os.listdir(os.path.join(path, class_)) if not img.startswith('.')]\n",
    "            lab = [class_ for i in os.listdir(os.path.join(path, class_)) if not i.startswith('.')]\n",
    "            labels = labels+lab\n",
    "            #print(dat)\n",
    "            data = data + dat\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "def initCustomDataGenerators(factor=1, shuffle_train=True):\n",
    "    global train_data, train_labels, val_data, val_labels, train_datagen, validation_datagen, train_generator, validation_generator, nb_train_samples, nb_validation_samples\n",
    "    # prepare data augmentation configuration\n",
    "    \n",
    "    train_data, train_labels = Read_Directory(TRAIN_DIR)\n",
    "    val_data, val_labels = Read_Directory(TEST_DIR)\n",
    "\n",
    "    print('Training samples: {}'.format(len(train_data)))\n",
    "    print('Validation samples: {}'.format(len(val_data)))\n",
    "\n",
    "    train_datagen = FileDataGen(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        aug_mode = 'StrongAug')\n",
    "\n",
    "    validation_datagen = FileDataGen(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        fill_mode='reflect')\n",
    "\n",
    "    train_generator = train_datagen.flow_from_filelist(\n",
    "        train_data, \n",
    "        train_labels,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=shuffle_train)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_filelist(\n",
    "        val_data,\n",
    "        val_labels,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_train_samples = len(train_data)\n",
    "    nb_validation_samples = len(val_data)\n",
    "    \n",
    "    print(nb_train_samples,nb_validation_samples)\n",
    "    \n",
    "def finetune_model(\n",
    "    tune_level = 0, \n",
    "    lr = 0.001, \n",
    "    factor = 1, \n",
    "    verbose = 0, \n",
    "    callbacks = [], \n",
    "    epochs = 10, \n",
    "    spe_factor = 1, \n",
    "    loss = \"binary_crossentropy\", \n",
    "    gen = 'default', \n",
    "    split_1 = None,\n",
    "    split_2 = None,\n",
    "    radam_wu_steps = None,\n",
    "    optimizer = 'adam'):\n",
    "    \n",
    "    if gen == 'default':\n",
    "        initDataGenerators(factor)\n",
    "    elif gen == 'custom':\n",
    "        initCustomDataGenerators(factor)\n",
    "    else:\n",
    "        initNoneDataGenerators()\n",
    "    \n",
    "    K.set_learning_phase(1)\n",
    "    \n",
    "    p = round((1-tune_level)*len(model.layers))\n",
    "\n",
    "    for layer in model.layers[:p]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[p:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    if tune_level == 0:\n",
    "        for layer in model.layers[-CUSTOM_LAYERS_NUM:]:\n",
    "            layer.trainable = True\n",
    "        for layer in model.layers[:-CUSTOM_LAYERS_NUM]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        model.compile(optimizer=Adam(lr), loss=loss, metrics=[categorical_accuracy])\n",
    "    elif optimizer == 'sgd':\n",
    "        model.compile(optimizer=SGD(lr=lr, momentum=0.9, nesterov=True), loss=loss, metrics=[categorical_accuracy])\n",
    "    elif optimizer == 'radam':\n",
    "        model.compile(optimizer=Lookahead(RAdam(learning_rate=lr)), loss=loss, metrics=[categorical_accuracy])\n",
    "    else:\n",
    "        model.compile(optimizer=Adam_dlr(split_1, split_2, lr), loss=loss, metrics=[categorical_accuracy])\n",
    "    #model.compile(optimizer=Nadam(lr), loss=loss, metrics=[categorical_accuracy])\n",
    "    #model.compile(optimizer=SGD(lr=lr, momentum=0.9, decay=0.00001, nesterov=True), loss=loss, metrics=[categorical_accuracy])\n",
    "    #model.compile(optimizer=Adadelta(), loss=loss, metrics=[categorical_accuracy])\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        #generator_wrapper(train_generator),\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size * spe_factor,\n",
    "        epochs=epochs,\n",
    "        #validation_data=generator_wrapper(validation_generator),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose)\n",
    "                  \n",
    "    #plot_training(history)\n",
    "    K.set_learning_phase(0)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def error_class_rate(gen):\n",
    "    gen.reset()\n",
    "    \n",
    "#     predz = model.predict_generator(gen, steps=len(gen))\n",
    "#     y_pred = np.argmax(preds, axis=1)\n",
    "#     #print(confusion_matrix(gen.classes, y_pred))\n",
    "#     classification_report(gen.classes, y_pred, output_dict = True)\n",
    "    \n",
    "    preds = model.predict_generator(gen, steps = len(gen))\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    #print(y_pred)\n",
    "    y_true = gen.classes\n",
    "    y_err = {0:0,1:0,2:0,3:0,4:0,5:0}\n",
    "    err_num = 0\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        if pred != y_true[i]:\n",
    "            y_err[y_true[i]] = y_err[y_true[i]] + 1\n",
    "            err_num = err_num + 1\n",
    "            \n",
    "    return {'y_err': y_err, 'er': err_num/len(y_true), 'err_num': err_num}\n",
    "\n",
    "def miss_error_class_rate(gen):\n",
    "    gen.reset()\n",
    "    preds = model.predict_generator(gen, steps = len(gen))\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    #print(y_pred)\n",
    "    y_true = gen.classes\n",
    "    y_err = {0:0,1:0,2:0,3:0,4:0,5:0}\n",
    "    err_num = 0\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        if pred != y_true[i]:\n",
    "            y_err[pred] = y_err[pred] + 1\n",
    "            err_num = err_num + 1\n",
    "            \n",
    "    return {'y_err': y_err, 'er': err_num/len(y_true), 'err_num': err_num}\n",
    "\n",
    "def evaluate_dynamic_weights(gen, miss_error = False):\n",
    "    global class_weight\n",
    "    \n",
    "    ecr = None\n",
    "#     if miss_error:\n",
    "#         ecr = miss_error_class_rate(gen)\n",
    "#     else:\n",
    "#         ecr = error_class_rate(gen)\n",
    "\n",
    "    gen.reset()\n",
    "    preds = model.predict_generator(gen, steps=len(gen))\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    #print(confusion_matrix(gen.classes, y_pred))\n",
    "    class_weight = {0:0,1:0,2:0,3:0,4:0,5:0}\n",
    "    metrs = classification_report(gen.classes, y_pred, output_dict = True)\n",
    "    for cls, metr in class_weight.items():\n",
    "        #print(cls)\n",
    "        #print(class_weight[cls])\n",
    "        class_weight[cls] = metrs[str(cls)]['f1-score']\n",
    "    #class_weight = classification_report(gen.classes, y_pred, output_dict = True)[0:5]\n",
    "#     for cls, metr in class_weight.items():\n",
    "#         print(cls,metr['f1-score'])\n",
    "    #class_weight = {int(cls): float(1-metr['f1-score']) for cls, metr in class_weight.items()}\n",
    "    \n",
    "    #minerr = min(y_err.values())\n",
    "    #class_weight = {cls: float(count/ecr['err_num'])*6.0 for cls, count in ecr['y_err'].items()}\n",
    "    \n",
    "    #print('Error classes', y_err)\n",
    "    #majority = max(counter.values())\n",
    "    #return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "    #class_weight = get_class_weights(y_err)\n",
    "#     for i in range(5):\n",
    "#         if not i in class_weight:\n",
    "#             class_weight[i] = 1\n",
    "    print(class_weight)\n",
    "\n",
    "    #return np.mean(class_weight.values())\n",
    "    return 0\n",
    "    #print('New weights', class_weight)\n",
    "    \n",
    "def finetune_with_adj_weights(tune_level = 0, \n",
    "                               lr = 0.001, \n",
    "                               factor = 1, \n",
    "                               callbacks = [], \n",
    "                               epochs = 10,\n",
    "                               miss_error = False,\n",
    "                               verbose = 1,\n",
    "                               spe_factor = 1, \n",
    "                               loss = \"binary_crossentropy\", \n",
    "                               gen = 'default',\n",
    "                               epochs_per_super = 1,\n",
    "                               save_models = True,\n",
    "                               optimizer = 'adam'):\n",
    "    trainhist = {}\n",
    "    for i in range(epochs):\n",
    "        print('_____________ SUPER EPOCH #',i)\n",
    "        newhist = finetune_model(tune_level, lr, factor, verbose, callbacks, epochs_per_super, spe_factor, loss, gen, optimizer)\n",
    "        datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "        generator = datagen.flow_from_directory(\n",
    "            TRAIN_DIR,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "        er = evaluate_dynamic_weights(generator, miss_error)\n",
    "        if save_models:\n",
    "            model.save('model_se'+str(i)+'_er'+str(np.round(er, 2))+'.hdf5')\n",
    "        \n",
    "        trainhist = mergeDicts(trainhist,newhist.history)\n",
    "        print('_____________ error rate = ', er)\n",
    "    plot_training(trainhist)\n",
    "    if save_models:\n",
    "        plt.savefig('models_hist.png')\n",
    "    #print(class_weight)\n",
    "    \n",
    "def confMatrix(gen):\n",
    "    gen.reset()\n",
    "    Y_pred = model.predict_generator(gen, steps=len(gen))\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    #print(confusion_matrix(gen.classes, y_pred))\n",
    "    seaborn.heatmap(confusion_matrix(gen.classes, y_pred))\n",
    "    print('Classification Report')\n",
    "    target_names = ['AD', 'CPOX', 'CS', 'HIVES', 'NE', 'P', 'RM']\n",
    "    print(classification_report(gen.classes, y_pred, target_names=target_names))\n",
    "    \n",
    "def prepare_image(image):\n",
    "    # if the image mode is not RGB, convert it\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # resize the input image and preprocess it\n",
    "    image = image.resize((img_width, img_height))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "    \n",
    "def predict_tta(datagen, model, samples, n_examples):\n",
    "    # convert image into dataset\n",
    "    # samples = np.expand_dims(image, 0)\n",
    "    # prepare iterator\n",
    "    it = datagen.flow(samples, batch_size=n_examples, shuffle=False)\n",
    "    # make predictions for each augmented image\n",
    "    yhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
    "    # sum across predictions\n",
    "    mean_probs = np.mean(yhats, axis=0)\n",
    "    # argmax across classes\n",
    "    return mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(0)\n",
    "\n",
    "img_width, img_height = 299, 299\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1 = base_model.layers[228].output\n",
    "x1 = model.layers[228].output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "#feature_extractor = Model(inputs=base_model.input, outputs=x1)\n",
    "feature_extractor = Model(inputs=model.input, outputs=x1)\n",
    "\n",
    "batch_size = 1\n",
    "initCustomDataGenerators(1, shuffle_train=False)\n",
    "#initNoneDataGenerators(shuffle_train=False)\n",
    "aug_n = 1\n",
    "\n",
    "y = np.tile(train_generator.classes,aug_n)\n",
    "X = feature_extractor.predict_generator(train_generator, steps = nb_train_samples // batch_size * aug_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier(n_estimators=1000, oob_score=True, min_samples_split=10, criterion='gini', bootstrap=True, random_state=5)\n",
    "clf = SVC(kernel='linear', C=5, decision_function_shape='ovo', probability=True)\n",
    "\n",
    "clf.fit(X, y)\n",
    "#clf = OneClassSVM(kernel='linear').fit(X)\n",
    "\n",
    "#predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = 0\n",
    "# rc = clf.predict(X)\n",
    "# for c in rc:\n",
    "#     if c<0:\n",
    "#         r = r + 1\n",
    "        \n",
    "# print(r/len(rc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator.reset()\n",
    "y_test = validation_generator.classes\n",
    "X_test = feature_extractor.predict_generator(validation_generator, steps = nb_validation_samples // batch_size)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "seaborn.heatmap(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "extra_validation_generator = extra_validation_datagen.flow_from_directory(\n",
    "    'valid',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "extra_nb_validation_samples = extra_validation_generator.samples\n",
    "#validation_generator.reset()\n",
    "y_test = extra_validation_generator.classes\n",
    "X_test = feature_extractor.predict_generator(extra_validation_generator, steps = extra_nb_validation_samples // batch_size)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "seaborn.heatmap(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "for i,l in enumerate(base_model.layers):\n",
    "    print(i,l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_LAYERS_NUM = 3\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "#x = base_model.layers[196].output\n",
    "#x = base_model.layers[164].output\n",
    "#x = base_model.output\n",
    "# x = base_model.layers[228].output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# # x = Dropout(0.3)(x)\n",
    "# x = Dense(128, activation='elu')(x)\n",
    "# #x = Dense(30, activation='elu')(x)\n",
    "# predictions = Dense(6, activation='sigmoid')(x)\n",
    "x1 = base_model.layers[228].output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "x1 = Dense(128, activation='elu')(x1)\n",
    "#x1 = Dense(6, activation='sigmoid')(x1)\n",
    "\n",
    "# x2 = base_model.layers[196].output\n",
    "# x2 = GlobalMaxPooling2D()(x2)\n",
    "# x2 = Dense(128, activation='elu')(x2)\n",
    "# #x2 = Dense(6, activation='sigmoid')(x2)\n",
    "\n",
    "# x3 = base_model.layers[228].output\n",
    "# x3 = GlobalMaxPooling2D()(x3)\n",
    "# x3 = Dense(128, activation='elu')(x3)\n",
    "# #x3 = Dense(6, activation='sigmoid')(x3)\n",
    "\n",
    "# x = Minimum()([x1,x2,x3])\n",
    "#predictions = Dense(6, activation='linear', kernel_regularizer=l2(0.01))(x1)\n",
    "predictions = Dense(7, activation='sigmoid')(x1)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "for i,l in enumerate(model.layers):\n",
    "    print(i,l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initCustomDataGenerators(1)\n",
    "#class_weight = get_class_weights(train_generator.classes)\n",
    "class_weight = {0:1, 1:1, 2:1, 3:1, 4:1, 5:1, 6:1}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "plt.figure(figsize=(14, 18))\n",
    "for k, (img, lbl) in enumerate(zip(x_batch, y_batch)):\n",
    "    plt.subplot(6, 5, k+1)\n",
    "    plt.imshow((img + 1) / 2)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(train_generator))\n",
    "print(train_generator.index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, restore_best_weights=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=0, verbose=1, mode='auto', min_delta=0.01, cooldown=2, min_lr=0.000001)\n",
    "histplot = PlotLossesCallback(plot_extrema=False)\n",
    "\n",
    "# finetune_with_adj_weights( tune_level = 0, \n",
    "#                            epochs_per_super = 2,\n",
    "#                            lr = 0.001, \n",
    "#                            factor = 1,\n",
    "#                            verbose = 1,\n",
    "#                            callbacks = [], \n",
    "#                            epochs = 5,\n",
    "#                            spe_factor = 1,\n",
    "#                            loss = losses.binary_crossentropy, \n",
    "#                            gen = 'custom', \n",
    "#                            optimizer = 'adam')\n",
    "\n",
    "finetune_model(tune_level = 0, \n",
    "               lr = 0.001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 40,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom',\n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('top_new_3_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = klm('top_new_3_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_with_adj_weights( tune_level = 0, \n",
    "                           epochs_per_super = 2,\n",
    "                           lr = 0.001, \n",
    "                           factor = 1,\n",
    "                           verbose = 1,\n",
    "                           callbacks = [], \n",
    "                           epochs = 5,\n",
    "                           spe_factor = 1,\n",
    "                           loss = losses.binary_crossentropy, \n",
    "                           gen = 'custom', \n",
    "                           optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_with_adj_weights( tune_level = 0, \n",
    "                           epochs_per_super = 2,\n",
    "                           lr = 0.001, \n",
    "                           factor = 1,\n",
    "                           verbose = 1,\n",
    "                           callbacks = [], \n",
    "                           epochs = 4,\n",
    "                           spe_factor = 1,\n",
    "                           loss = losses.binary_crossentropy, \n",
    "                           gen = 'custom', \n",
    "                           optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0:1, 1:1, 2:1, 3:1, 4:1, 5:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(tune_level = 0.1, \n",
    "               lr = 0.001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 40,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom', \n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.layers[-3].output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='elu')(x)\n",
    "#x = Dense(30, activation='elu')(x)\n",
    "predictions = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(tune_level = 0, \n",
    "               lr = 0.001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 40,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom',\n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(tune_level = 0.2, \n",
    "               lr = 0.0001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 40,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom', \n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.layers[-3].output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='elu')(x)\n",
    "#x = Dense(30, activation='elu')(x)\n",
    "predictions = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(tune_level = 0.4, \n",
    "               lr = 0.0001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 40,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom',\n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.layers[-3].output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='elu')(x)\n",
    "#x = Dense(30, activation='elu')(x)\n",
    "predictions = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "finetune_model(tune_level = 0.1, \n",
    "               lr = 0.001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 4,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom', \n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(tune_level = 0, \n",
    "               lr = 0.001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 40,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom',\n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('top_new_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def exp_decay(lr = 0.1, k = 0.1):\n",
    "    return lambda epoch: lr * math.exp(-k*epoch)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = LearningRateScheduler(exp_decay(lr = 0.001, k = 0.1))\n",
    "\n",
    "finetune_model(tune_level = 0, \n",
    "               lr = 0.001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es,lrs], \n",
    "               epochs = 20,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom', \n",
    "               optimizer = 'sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('top_new_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(tune_level = 0, \n",
    "               lr = 0.0001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 40,\n",
    "               spe_factor = 1,\n",
    "               loss = losses.binary_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom',\n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, restore_best_weights=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=0, verbose=1, mode='auto', min_delta=0.01, cooldown=2, min_lr=0.000001)\n",
    "histplot = PlotLossesCallback(plot_extrema=False)\n",
    "\n",
    "finetune_model(tune_level = 0.1, \n",
    "               lr = 0.0001, \n",
    "               factor = 1, \n",
    "               verbose = 1,\n",
    "               callbacks = [es], \n",
    "               epochs = 20,\n",
    "               spe_factor = 1,\n",
    "               #loss = losses.categorical_crossentropy, \n",
    "               #loss = WeightedCategoricalCrossentropy(np.array([[0,1,2,2,2,1],[1,0,1,1.5,1,2],[2,1,0,1,1,1],[2,1.5,1,0,1.5,1],[2,1,1,1.5,0,1],[1,2,1,1,1,0]])),\n",
    "               gen = 'custom', \n",
    "               optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fine_new_8_best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "    \n",
    "confMatrix(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = klm('fine_new_8_best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = model.layers[228].output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "#feature_extractor = Model(inputs=base_model.input, outputs=x1)\n",
    "feature_extractor = Model(inputs=model.input, outputs=x1)\n",
    "\n",
    "initCustomDataGenerators(1)\n",
    "Xtr = feature_extractor.predict_generator(train_generator, steps = nb_train_samples // batch_size)\n",
    "Xte = feature_extractor.predict_generator(validation_generator, steps = nb_validation_samples // batch_size)\n",
    "X = np.concatenate((Xtr, Xte), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_input = Input(shape=(768,))\n",
    "x = Dense(10, activation='elu', activity_regularizer=l1(10e-5))(ae_input)\n",
    "# x = Dense(128, activation='elu')(ae_input)\n",
    "# x = Dense(64, activation='elu')(x)\n",
    "# x = Dense(2, activation='elu')(x)\n",
    "# x = Dense(64, activation='elu')(x)\n",
    "# x = Dense(128, activation='elu')(x)\n",
    "ae_output = Dense(768, activation='sigmoid')(x)\n",
    "autoencoder = Model(inputs=ae_input, outputs=ae_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initCustomDataGenerators(1)\n",
    "    \n",
    "K.set_learning_phase(1)\n",
    "\n",
    "# for layer in autoencoder.layers[:-2]:\n",
    "#     layer.trainable = False\n",
    "for layer in autoencoder.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "autoencoder.compile(optimizer=Adam(0.0001), loss=losses.binary_crossentropy, metrics=[])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1, restore_best_weights=True)\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X[:600],\n",
    "    X[:600],\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=100,\n",
    "    validation_data=(X[600:], X[600:]),\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[],\n",
    "    verbose=1)\n",
    "\n",
    "#plot_training(history)\n",
    "K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(1)\n",
    "history = autoencoder.fit(\n",
    "    X[:600],\n",
    "    X[:600],\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=(X[600:], X[600:]),\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[es],\n",
    "    verbose=1)\n",
    "\n",
    "#plot_training(history)\n",
    "K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.evaluate(X[600:], X[600:], steps=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.evaluate(X[:600], X[:600], steps=nb_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "xv_generator = xv_datagen.flow_from_directory(\n",
    "    'valid',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "Xv = feature_extractor.predict_generator(xv_generator, steps = xv_generator.samples // batch_size)\n",
    "\n",
    "autoencoder.evaluate(Xv, Xv, steps=xv_generator.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xwrong_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "xwrong_generator = xwrong_datagen.flow_from_directory(\n",
    "    'wrongdata/ne',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "Xwrong = feature_extractor.predict_generator(xwrong_generator, steps = xwrong_generator.samples // batch_size)\n",
    "\n",
    "autoencoder.evaluate(Xwrong, Xwrong, steps=xwrong_generator.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model as klm\n",
    "\n",
    "# model1 = klm('model_se4_er0.02.hdf5')\n",
    "# model2 = klm('fine_new_4.hdf5')\n",
    "# model3 = klm('fine_new_5.hdf5')\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "#model = klm('./cnns/model-finetune-v1-04-0.05.hdf5')\n",
    "#model = klm('./finetunemodel_adj_w/model_se2_er0.14.hdf5')\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# validation_datagen = ImageDataGenerator(\n",
    "#     preprocessing_function=preprocess_input,\n",
    "#     #rotation_range=30,\n",
    "#     #width_shift_range=0.01,\n",
    "#     #height_shift_range=0.01,\n",
    "#     #shear_range=0.01,\n",
    "#     #zoom_range=0.1,\n",
    "#     #horizontal_flip=True,\n",
    "#     fill_mode='reflect')\n",
    "\n",
    "# validation_generator = validation_datagen.flow_from_directory(\n",
    "#     'valid',\n",
    "#     target_size=(img_width, img_height),\n",
    "#     batch_size=16,\n",
    "#     class_mode='categorical',\n",
    "#     shuffle=False)\n",
    "\n",
    "tdg = ImageDataGenerator(\n",
    "    rotation_range=60,\n",
    "    #width_shift_range=0.1,\n",
    "    #height_shift_range=0.1,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "n_examples = 10\n",
    "\n",
    "score = 0\n",
    "\n",
    "test_db = glob('./valid/atopic_dermatitis/*.*') + glob('./valid/hives/*.*') + glob('./valid/nummular_eczema/*.*')\n",
    "test_db_classes = [0]*len(glob('./valid/atopic_dermatitis/*.*')) + [3]*len(glob('./valid/hives/*.*')) + [4]*len(glob('./valid/nummular_eczema/*.*'))\n",
    "\n",
    "for i, imurl in enumerate(test_db):\n",
    "    image = prepare_image(PIL.Image.open(imurl))\n",
    "    pred = predict_tta(tdg, model, image, n_examples)\n",
    "#     pred1 = predict_tta(tdg, model1, image, n_examples)\n",
    "#     pred2 = predict_tta(tdg, model2, image, n_examples)\n",
    "#     pred3 = predict_tta(tdg, model3, image, n_examples)\n",
    "#     pred = np.average([pred1, pred2, pred3], axis=0)\n",
    "    print(imurl, np.round(pred, 2))\n",
    "    #print(test_db_classes[i], np.equal(np.argmax(pred),test_db_classes[i]))\n",
    "    score = score + np.equal(np.argmax(pred),test_db_classes[i])\n",
    "    \n",
    "print('tta score ', score/len(test_db))\n",
    "\n",
    "tdg = ImageDataGenerator()\n",
    "n_examples = 1\n",
    "score = 0\n",
    "\n",
    "for i, imurl in enumerate(test_db):\n",
    "    image = prepare_image(PIL.Image.open(imurl))\n",
    "    pred = predict_tta(tdg, model, image, n_examples)\n",
    "#     pred1 = predict_tta(tdg, model1, image, n_examples)\n",
    "#     pred2 = predict_tta(tdg, model2, image, n_examples)\n",
    "#     pred3 = predict_tta(tdg, model3, image, n_examples)\n",
    "#     pred = np.average([pred1, pred2, pred3], axis=0)\n",
    "    print(imurl, np.round(pred, 2))\n",
    "    #print(test_db_classes[i], np.equal(np.argmax(pred),test_db_classes[i]))\n",
    "    score = score + np.equal(np.argmax(pred),test_db_classes[i])\n",
    "    \n",
    "print('no-tta score ', score/len(test_db))\n",
    "\n",
    "# print(validation_generator.filenames)\n",
    "# print(model.evaluate_generator(validation_generator))\n",
    "# preds = model.predict_generator(validation_generator)\n",
    "# print(np.round(preds, 2))\n",
    "# #y_pred = np.round(preds, 2)\n",
    "# y_pred = np.argmax(preds, axis=1)\n",
    "# print(y_pred)\n",
    "# for i, pred in enumerate(y_pred):\n",
    "#     print(i,pred)\n",
    "# y_true = validation_generator.classes\n",
    "# print(y_pred,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaidml-venv",
   "language": "python",
   "name": "plaidml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
